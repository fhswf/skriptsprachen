{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling von Stellenanzeigen mit `scrapy`\n",
    "\n",
    "Mithilfe des Pakets `scrapy` wollen wir heute Stellenangebote von [stellenanzeigen.de](https://www.stellenanzeigen.de) crawlen und in einer *MongoDB* speichern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class Job(scrapy.Item):\n",
    "    \"\"\"Klasse zur Speicherung der Stellenangebote\"\"\"\n",
    "    header = scrapy.Field()\n",
    "    text = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    \n",
    "class JobSpider(scrapy.Spider):\n",
    "    \"\"\"Crawler f√ºr die Seite stellenanzeigen.de\"\"\"\n",
    "    \n",
    "    name = \"stellenanzeigen.de\"\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_FAIL_ON_DATALOSS': False,\n",
    "        'DOWNLOAD_DELAY': 0.5,\n",
    "        'ITEM_PIPELINES': {\n",
    "            '__main__.MongoPipeline': 100,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    link_extractor = LinkExtractor(restrict_css=('a.position-link'))\n",
    "    iframe_extractor = LinkExtractor(tags=('iframe'), attrs=('src'), restrict_css=('iframe.jobview-iframe'))\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://www.stellenanzeigen.de/stellenangebote/it/\",\n",
    "            #\"https://www.stellenanzeigen.de/job/softwareentwickler-python-m-w-d-2896251/\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")\n",
    "        links = self.link_extractor.extract_links(response)\n",
    "        iframes = self.iframe_extractor.extract_links(response)\n",
    "        \n",
    "        for link in iframes:\n",
    "            print(f\"link: {link}\")\n",
    "            yield scrapy.Request(link.url, priority=100, callback=self.parse_iframe)\n",
    "            \n",
    "        for link in links:\n",
    "            print(f\"link: {link}\")\n",
    "            yield scrapy.Request(link.url)\n",
    "            \n",
    "    def parse_iframe(self, response):\n",
    "        for job in response.css('html'):\n",
    "            referer = response.request.headers.get('Referer', None).decode('latin1')\n",
    "            print(f\"job: {job.get()}\")\n",
    "            item = Job()\n",
    "            item['header'] = job.css('h1 ::text').get()\n",
    "            item['text'] = job.get().strip()\n",
    "            item['link'] = referer\n",
    "            yield item    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JSONPipeline():\n",
    "    \"\"\"Ablage der gecrawlten Items als JSON-Datei\"\"\"\n",
    "    \n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('jobs.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "        \n",
    "    def process_item(self, item, spider):\n",
    "        print(f\"processing: {item}\")\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "class MongoPipeline():\n",
    "    \"\"\"Ablage der gecrawlten Items in einer MongoDB\"\"\"\n",
    "    \n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess({\n",
    "    'MONGO_URI': \"mongodb://172.22.0.2:27017\",\n",
    "    'MONGO_DATABASE': 'jobs',\n",
    "    'DOWNLOAD_DELAY': 0.5,\n",
    "    #'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "}\n",
    ")\n",
    "\n",
    "process.crawl(JobSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
