{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ELIZA** on Steroids: Texterzeugung mit GPT-2\n",
    "\n",
    "Eines der spannendsten und herausforderndsten Themen ist die Erzeugung von Texten mittels künstlicher Intelligenz. \n",
    "\n",
    "Aktuell gibt es mit GPT-2 ein Sprachmodell, das so gute künstliche Texte erzeugt, dass die Firma OpenAI es nur in einer \"abgespeckten\" Version [veröffentlicht](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "Mit diesem Sprachmodell benötigt man nur wenige Zeilen Code, um einen englischen Text fortzusetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1102 22:50:51.707658 139965096539968 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/jupyter/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I1102 22:50:51.710873 139965096539968 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/jupyter/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I1102 22:50:52.358298 139965096539968 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/jupyter/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "I1102 22:50:52.361623 139965096539968 configuration_utils.py:168] Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "I1102 22:50:53.012093 139965096539968 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/jupyter/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    }
   ],
   "source": [
    "# Lade Tokenizer und Modell\n",
    "tok = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This app is pretty\n",
      " simple and there's nothing I want to write here about other than that I just like what they're doing here and this is really what they're trying to accomplish."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This app is pretty simple and there's nothing I want to write here about other than that I just like what they're doing here and this is really what they're trying to accomplish.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pred(text, model, tok, p=0.7):\n",
    "    \"\"\" Setze text mit Hilfe von model um ein Token fort.\n",
    "    Wähle dabei zufällig aus so vielen Token, dass ihre Gesamtwahrscheinlichkeit p beträgt.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Zerlege Text in Tokens und erzeuge Eingabetensor\n",
    "    input_ids = torch.tensor(tok.encode(text)).unsqueeze(0)\n",
    "    \n",
    "    # Wende das Modell an und erzeuge einen Vektor mit den Wahrscheinlichkeiten für das nächste Wort\n",
    "    logits = model(input_ids)[0][:, -1]\n",
    "    probs = F.softmax(logits, dim=-1).squeeze()\n",
    "    \n",
    "    # Sortiere Kandidaten absteigend nach Wahrscheinlichkeit\n",
    "    idxs = torch.argsort(probs, descending=True)\n",
    "    res, cumsum = [], 0.\n",
    "    \n",
    "    # Wähle so viele Kandidaten, dass die Summe ihrer Wahrscheinlichkeiten p beträgt\n",
    "    for idx in idxs:\n",
    "        res.append(idx)\n",
    "        cumsum += probs[idx]\n",
    "        if cumsum > p:\n",
    "            # Wähle aus den Kandidaten einen zufälligen aus\n",
    "            pred_idx = idxs.new_tensor([choice(res)])\n",
    "            break\n",
    "    \n",
    "    # Wandle Ergebnis in Text um\n",
    "    pred = tok.convert_ids_to_tokens(int(pred_idx))\n",
    "    return tok.convert_tokens_to_string(pred)\n",
    "\n",
    "\n",
    "def generate_continuation(text):\n",
    "    \"\"\" Setze Text so lange fort, bis ein Textende generiert wird oder fünf Sätze gebildet wurden. \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    print(text)\n",
    "\n",
    "    while text.count(\".\") < 5:\n",
    "        res = get_pred(text, model, tok)\n",
    "        if res == \"<|endoftext|>\":\n",
    "            break\n",
    "        print(res, end=\"\")\n",
    "        text += res\n",
    "    return text\n",
    "\n",
    "generate_continuation(\"This app is pretty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote,  previously unexplored valley, in the Andes Mountains.  Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      " \"That was really exciting to see that there were hundreds of different species living in this remote part of the world,\" explains Christopher Matthews, one of the scientists involved in the research. \"It really gives you a feeling that these little things could live out in nature for many, many, many, many years.\"\n",
      "\n",
      " And of course, not all unicorns have unique uses."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In a shocking finding, scientist discovered a herd of unicorns living in a remote,  previously unexplored valley, in the Andes Mountains.  Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \"That was really exciting to see that there were hundreds of different species living in this remote part of the world,\" explains Christopher Matthews, one of the scientists involved in the research. \"It really gives you a feeling that these little things could live out in nature for many, many, many, many years.\"\\n\\n And of course, not all unicorns have unique uses.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, \n",
    "previously unexplored valley, in the Andes Mountains. \n",
    "Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\"\"\n",
    "\n",
    "generate_continuation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I guess the problem is that you need to start a separate thread for each connection and call serverSocket.accept() in a loop to accept more than one connection. It is not a problem to have more than\n",
      " one thread running. However, some implementations that implement that provide methods that only accept incoming connections will fail because their data doesn't actually pass. In other cases, such as multi-connection communications, that implementation should create separate queues."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I guess the problem is that you need to start a separate thread for each connection and call serverSocket.accept() in a loop to accept more than one connection. It is not a problem to have more than one thread running. However, some implementations that implement that provide methods that only accept incoming connections will fail because their data doesn't actually pass. In other cases, such as multi-connection communications, that implementation should create separate queues.\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"I guess the problem is that you need to start a separate thread for each\n",
    "connection and call serverSocket.accept() in a loop to accept more than one connection.\n",
    "It is not a problem to have more than\"\"\"\n",
    "\n",
    "generate_continuation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As the above samples show, our model is capable of generating samples from a variety of  prompts that feel close to human quality and show coherence over a page or more of text.  Nevertheless, we are not satisfied with our answer to the second question of what does each background pose. This begs the question of how an ensemble can form one specific individual context or interpret its sounds. It has become an obvious problem that every time a message in the future \"knew what he signed up for\" , there are patterns emerging of increased alarm to expectations and habituation of these auditory cues, regardless of how an individual contexts themselves do not affect these things. These data are summarized below, summarizing the complex associations observed among one person\\'s name and track as seen through various radio displays, phonograph radios, t-shirts, balloons, televisions, YouTube, social media, laptops, digital radio (indicator boards), notebook paper, cards, old magazine posters, software books, reams of playlists, telephonograms, ping-pong balls, reading textbooks, taking up other skills, discussing oneself and focusing on others, to name a few.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"As the above samples show, our model is capable of generating samples from a variety of \n",
    "prompts that feel close to human quality and show coherence over a page or more of text. \n",
    "Nevertheless\"\"\"\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "while text.count(\".\") < 5:\n",
    "    text += get_pred(text, model, tok)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am committed to work hard in everything we do. While working at Max here I realize we have lost one step here,\" Klicker added.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am\"\n",
    "while text.count(\".\") < 2:\n",
    "    text += get_pred(text, model, tok, 0.8)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are prohibited from changing or destroying documents in the vehicle at a rally in Kansas. By being a law-abiding citizen and owning a valid concealed handgun license you do not hereby warrant any type of evidence or expert testimony about this case to any FBI or law enforcement agency in Missouri.\"'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"You are\"\n",
    "while text.count(\".\") < 2:\n",
    "    text += get_pred(text, model, tok, 0.8)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dad is no stranger to building dynamic muscle, and the season's in no small part because of that. He'll pull up in the first couple of games, so his workouts will be limited, but if you don't feel he's in a rhythm he'll definitely take on that load.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Dad is\"\n",
    "while text.count(\".\") < 2:\n",
    "    text += get_pred(text, model, tok, 0.6)\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
