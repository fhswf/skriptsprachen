{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning auf Textdaten: GermEval 2018\n",
    "\n",
    "*GermEval* – für German Evaluation – ist ein jährlicher Wettbewerb im Bereich Natural Language Processing für deutschsprachige Texte (s. [https://germeval.github.io/](https://germeval.github.io/)).\n",
    "\n",
    "Im Jahr 2018 ging es um die Erkennung von Beleidigungen in Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Record = namedtuple('Record', [ 'text', 'primary_label', 'secondary_label' ])\n",
    "\n",
    "with open('germeval2018.training.txt', 'r') as file:\n",
    "    training_data = [ Record(*line[:-1].split('\\t')) for line in file ]\n",
    "\n",
    "with open('germeval2018.test.txt', 'r') as file:\n",
    "    test_data = [ Record(*line[:-1].split('\\t')) for line in file ]\n",
    "\n",
    "training_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([ (record.primary_label, record.secondary_label) for record in training_data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([ (record.primary_label, record.secondary_label) for record in test_data ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing der Tweets\n",
    "\n",
    "Für die weitere Verarbeitung wollen wir Twitter Handles (`@username`) löschen und das Hashtag-Zeichen entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\" Preprocess and tokenize a tweet. \"\"\"\n",
    "    \n",
    "    # remove handles, i.e. @username\n",
    "    text = re.sub('\\@\\w+', '', text)\n",
    "    # remove hashtags, quotes, etc.\n",
    "    text = re.sub('[\\#\"\\']+', '', text)\n",
    "    text = text.replace('-', ' ')\n",
    "    return text\n",
    "\n",
    "clean_tweet(training_data[4].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umwandlung in Tensoren\n",
    "\n",
    "Für die weitere Verarbeitung mit `scikit-learn` wandeln wir die Daten in eine passende Tensorstruktur um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "def convert_data(input):\n",
    "    \"\"\" Convert data array into tensor structure. \"\"\"\n",
    "    data   = np.array([ clean_tweet(record.text) for record in input ])\n",
    "    coarse = np.array([ record.primary_label for record in input ])\n",
    "    fine   = np.array([ record.secondary_label for record in input ])\n",
    "    \n",
    "    return { 'data': data, 'coarse': coarse, 'fine': fine }\n",
    "\n",
    "train = convert_data(training_data)\n",
    "test  = convert_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(classifier):\n",
    "    predicted = classifier.predict(test['data'])\n",
    "    print(f\"Confusion matrix:\\n{metrics.confusion_matrix(test['coarse'], predicted)}\")\n",
    "    print(f\"{metrics.classification_report(test['coarse'], predicted)}\")\n",
    "    return np.mean(predicted == test['coarse'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entsprechend dem Cheat-Sheet probieren wir es zunächst mit einem `LinearSVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "text_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "text_classifier.fit(train['data'], train['coarse'])\n",
    "evaluate(text_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Kandidat für Text ist `NaiveBayes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "bayes_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(bayes_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Vergleich `KNeighbors` und `DecisionTree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "text_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', KNeighborsClassifier()),\n",
    "])\n",
    "\n",
    "text_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(text_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', DecisionTreeClassifier()),\n",
    "])\n",
    "\n",
    "text_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(text_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tatsächlich funktioniert bisher der `NaiveBayes` am besten. Allerdings ist der Recall noch nicht so besonders gut. Die Frage ist, ob hier z.B. Word-Embeddings helfen. Wir importieren dazu von der Uni Heidelberg auf deutschen Twitter-Nachrichten trainierte Word Embeddings (s. [Download](https://www.cl.uni-heidelberg.de/english/research/downloads/resource_pages/GermanTwitterEmbeddings/GermanTwitterEmbeddings_data.shtml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('twitter-de_d100_w5_min10.bin', binary=True)\n",
    "print(model.most_similar(positive='Merkel', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_vector('Merkel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings wurden populär, weil Bedeutungszusammenhänge abbilden können, also etwa\n",
    "```\n",
    "'Mutter' - 'Frau' + 'Mann' = ' Vater'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vater = model.get_vector('Mann') - model.get_vector('Frau') + model.get_vector('Mutter')\n",
    "model.distances(vater, ('Vater', 'Mutter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance('Merkel', 'Bundeskanzlerin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance('Fachhochschule', 'Bundeskanzler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance('Fachhochschule', 'FH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance('Flüchtlinge', 'Nichtdeutsche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distance('Fachhochschule', 'Nichtdeutsche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import nltk\n",
    "\n",
    "class EmbeddingVectorizer(BaseEstimator):\n",
    "    \"\"\"Convert a collection of text documents to a matrix of vectors created from word embeddings \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format('twitter-de_d100_w5_min10.bin', binary=True)\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        \"\"\"Nothing to do here, we use a pre-trained model. \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to embedding matrix by calculating the L2-normalized sum of the embeddings\n",
    "        of individual words.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        _X = []\n",
    "        for doc in raw_documents:\n",
    "            x = np.zeros(100)\n",
    "            for word in self.tokenizer.tokenize(doc):\n",
    "                try:\n",
    "                    x += self.model.get_vector(word)\n",
    "                except KeyError:\n",
    "                    #print(f\"ignoring {word} not in vocabulary\")\n",
    "                    pass\n",
    "                \n",
    "            x /= np.linalg.norm(x)\n",
    "            _X.append(x)\n",
    "        \n",
    "        X = np.array(_X)\n",
    "        return X\n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = EmbeddingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_svc_classifier = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "w2v_svc_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(w2v_svc_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "w2v_rf_classifier = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('clf', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "w2v_rf_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(w2v_rf_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "classifiers = [ \n",
    "    ('bayes', bayes_classifier),\n",
    "    ('svc', w2v_svc_classifier),\n",
    "    ('rf', w2v_rf_classifier)\n",
    "]\n",
    "\n",
    "voting_classifier = VotingClassifier(classifiers)\n",
    "\n",
    "voting_classifier.fit(train['data'], train['coarse']) \n",
    "evaluate(voting_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
